{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3b4cb9",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATASET_PATH = BASE_DIR / \"dataset_filtered\"\n",
    "LABEL_MAP = BASE_DIR / \"label_map.txt\"\n",
    "MODEL_NAME = BASE_DIR / \"crop_disease_model.keras\"\n",
    "EPOCHS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8979ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset folder not found at {DATASET_PATH}\\n\"\n",
    "        \"Please download the dataset and place it in the project root as 'dataset_filtered/'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac4a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=25,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d57a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6961d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16254 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26dd404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4058 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = val_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08aec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Label map saved to d:\\Lohith\\BTECH CODES\\Machine_Learning\\Project\\Crop-Disease\\label_map.txt\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = train_data.num_classes\n",
    "labels = train_data.class_indices\n",
    "with open(LABEL_MAP, \"w\") as f:\n",
    "    for label, idx in labels.items():\n",
    "        f.write(f\"{idx}:{label}\\n\")\n",
    "\n",
    "print(\"âœ… Label map saved to\", LABEL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c0730",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURE AND TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c6d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f20e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c809fd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,341</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m327,936\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             â”‚         \u001b[38;5;34m3,341\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,589,261</span> (9.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,589,261\u001b[0m (9.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331,277</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m331,277\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bba55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: np.float64(1.5628846153846154), 1: np.float64(1.5628846153846154), 2: np.float64(10.2484237074401), 3: np.float64(0.7346108650456477), 4: np.float64(1.5628846153846154), 5: np.float64(0.8182641965364479), 6: np.float64(1.6408237431859478), 7: np.float64(0.8823625210357744), 8: np.float64(0.9323696437790397), 9: np.float64(1.1123733917328222), 10: np.float64(0.29171901360422126), 11: np.float64(4.181631077952148), 12: np.float64(0.9821741494954378)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_data.classes),\n",
    "    y=train_data.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c568a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(MODEL_NAME, save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4765566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 796ms/step - accuracy: 0.6634 - loss: 1.1093 - val_accuracy: 0.8376 - val_loss: 0.5336 - learning_rate: 3.0000e-04\n",
      "Epoch 2/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 384ms/step - accuracy: 0.8123 - loss: 0.5804 - val_accuracy: 0.8509 - val_loss: 0.4403 - learning_rate: 3.0000e-04\n",
      "Epoch 3/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 386ms/step - accuracy: 0.8452 - loss: 0.4797 - val_accuracy: 0.8455 - val_loss: 0.4345 - learning_rate: 3.0000e-04\n",
      "Epoch 4/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 387ms/step - accuracy: 0.8627 - loss: 0.4217 - val_accuracy: 0.8825 - val_loss: 0.3319 - learning_rate: 3.0000e-04\n",
      "Epoch 5/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 387ms/step - accuracy: 0.8779 - loss: 0.3777 - val_accuracy: 0.8797 - val_loss: 0.3457 - learning_rate: 3.0000e-04\n",
      "Epoch 6/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 389ms/step - accuracy: 0.8848 - loss: 0.3526 - val_accuracy: 0.8879 - val_loss: 0.3226 - learning_rate: 3.0000e-04\n",
      "Epoch 7/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 387ms/step - accuracy: 0.8924 - loss: 0.3352 - val_accuracy: 0.9012 - val_loss: 0.2864 - learning_rate: 3.0000e-04\n",
      "Epoch 8/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 390ms/step - accuracy: 0.8972 - loss: 0.3056 - val_accuracy: 0.9054 - val_loss: 0.2674 - learning_rate: 3.0000e-04\n",
      "Epoch 9/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 388ms/step - accuracy: 0.9038 - loss: 0.2834 - val_accuracy: 0.9049 - val_loss: 0.2795 - learning_rate: 3.0000e-04\n",
      "Epoch 10/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 389ms/step - accuracy: 0.9020 - loss: 0.2894 - val_accuracy: 0.9051 - val_loss: 0.2787 - learning_rate: 3.0000e-04\n",
      "Epoch 11/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 390ms/step - accuracy: 0.9173 - loss: 0.2498 - val_accuracy: 0.9128 - val_loss: 0.2538 - learning_rate: 9.0000e-05\n",
      "Epoch 12/12\n",
      "\u001b[1m508/508\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 391ms/step - accuracy: 0.9214 - loss: 0.2366 - val_accuracy: 0.9140 - val_loss: 0.2469 - learning_rate: 9.0000e-05\n",
      "Final Training Accuracy: 0.921373188495636\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(\"Final Training Accuracy:\", history.history[\"accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f94ddc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final model saved\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_NAME)\n",
    "print(\"âœ… Final model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0f40fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "with open(LABEL_MAP, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, label = line.strip().split(\":\")\n",
    "        label_map[int(idx)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82658ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMEDIATION_DB = {\n",
    "    \"Potato___Early_blight\": {\n",
    "        \"organic\": [\n",
    "            \"Remove infected leaves\",\n",
    "            \"Spray neem oil (3 ml/L)\",\n",
    "            \"Improve air circulation\"\n",
    "        ],\n",
    "        \"chemical\": [\n",
    "            \"Spray Mancozeb (2.5 g/L)\",\n",
    "            \"Repeat every 7 days\"\n",
    "        ],\n",
    "        \"prevention\": [\n",
    "            \"Avoid overhead irrigation\",\n",
    "            \"Use disease-resistant varieties\",\n",
    "            \"Crop rotation\"\n",
    "        ],\n",
    "        \"safety\": [\n",
    "            \"Wear gloves & mask\",\n",
    "            \"Do not spray during noon\",\n",
    "            \"Keep away from children\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"Potato___Late_blight\": {\n",
    "        \"organic\": [\n",
    "            \"Destroy infected plants\",\n",
    "            \"Apply compost tea\"\n",
    "        ],\n",
    "        \"chemical\": [\n",
    "            \"Spray Metalaxyl (2 g/L)\",\n",
    "            \"Repeat every 5 days\"\n",
    "        ],\n",
    "        \"prevention\": [\n",
    "            \"Avoid water stagnation\",\n",
    "            \"Use certified seeds\"\n",
    "        ],\n",
    "        \"safety\": [\n",
    "            \"Use PPE\",\n",
    "            \"Avoid spraying before rain\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"Tomato___Early_blight\": {\n",
    "        \"organic\": [\n",
    "            \"Remove affected leaves\",\n",
    "            \"Use neem oil spray\"\n",
    "        ],\n",
    "        \"chemical\": [\n",
    "            \"Spray Chlorothalonil\",\n",
    "            \"Repeat every 7 days\"\n",
    "        ],\n",
    "        \"prevention\": [\n",
    "            \"Mulching\",\n",
    "            \"Avoid wet leaves\"\n",
    "        ],\n",
    "        \"safety\": [\n",
    "            \"Avoid inhalation\",\n",
    "            \"Wear gloves\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6162aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 13), dtype=float32, numpy=\n",
       "array([[1.03741854e-07, 2.15192676e-06, 6.48439936e-07, 2.08785437e-04,\n",
       "        7.98078690e-05, 9.97325420e-01, 6.60226988e-06, 1.92963460e-03,\n",
       "        2.79244932e-06, 1.59534375e-06, 2.55323044e-04, 6.22783148e-07,\n",
       "        1.86423902e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(MODEL_NAME)\n",
    "\n",
    "dummy_input = np.zeros((1, 224, 224, 3), dtype=np.float32)\n",
    "model(dummy_input)  # ğŸ‘ˆ IMPORTANT: call model directly, not predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a585647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blurry(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    variance = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "    threshold = 100.0  # You can adjust this threshold\n",
    "    return variance < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0699a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam(model, img_path, layer_name=None):\n",
    "    \"\"\"Generate a Grad-CAM heatmap overlay for the predicted class.\n",
    "\n",
    "    Fixes Keras graph issues by building the Grad-CAM model using the\n",
    "    MobileNetV2 base model's input graph (not Sequential's symbolic tensors).\n",
    "    \"\"\"\n",
    "    base_model = model.layers[0]  # MobileNetV2 (Functional)\n",
    "\n",
    "    # Pick a sensible conv layer if not provided\n",
    "    if layer_name is None:\n",
    "        for layer in reversed(base_model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                layer_name = layer.name\n",
    "                break\n",
    "        if layer_name is None:\n",
    "            # Fallback: any layer with 'conv' in its name\n",
    "            for layer in reversed(base_model.layers):\n",
    "                if \"conv\" in layer.name.lower():\n",
    "                    layer_name = layer.name\n",
    "                    break\n",
    "\n",
    "    if layer_name is None:\n",
    "        raise ValueError(\"No convolutional layer found in base model.\")\n",
    "\n",
    "    conv_layer = base_model.get_layer(layer_name)\n",
    "\n",
    "    # Build a Grad-CAM graph rooted at base_model.input to avoid tensor mismatch\n",
    "    x = base_model.output\n",
    "    for head_layer in model.layers[1:]:\n",
    "        x = head_layer(x)\n",
    "    preds = x\n",
    "\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=[conv_layer.output, preds],\n",
    "    )\n",
    "\n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0).astype(np.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        pred_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(pooled_grads * conv_outputs, axis=-1)\n",
    "\n",
    "    # Convert to numpy once, then use numpy/cv2 safely\n",
    "    heatmap = heatmap.numpy()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= (np.max(heatmap) + 1e-8)\n",
    "\n",
    "    heatmap = cv2.resize(heatmap, (224, 224))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    original = cv2.imread(img_path)\n",
    "    if original is None:\n",
    "        raise FileNotFoundError(f\"Could not read image from path: {img_path}\")\n",
    "    original = cv2.resize(original, (224, 224))\n",
    "\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(original, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    output_path = \"heatmap_output.jpg\"\n",
    "    cv2.imwrite(output_path, overlay)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disease(img_path, confidence_threshold=0.45):\n",
    "    if is_blurry(img_path):\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": \"Image is too blurry. Please retake photo.\",\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "    \n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    preds = model.predict(img_array)[0]\n",
    "    top_idx = np.argmax(preds)\n",
    "    confidence = float(preds[top_idx])\n",
    "    disease_name = label_map[top_idx]\n",
    "\n",
    "    # Epic 3: Unknown/Unclear Result Story\n",
    "    if confidence < confidence_threshold:\n",
    "        return {\n",
    "            \"status\": \"unknown\",\n",
    "            \"disease\": \"Unknown/Unclear\",\n",
    "            \"message\": \"Please consult a human expert or retake the photo.\",\n",
    "            \"confidence\": round(confidence * 100, 2)\n",
    "        }\n",
    "    \n",
    "    # Epic 3: Healthy Plant Story\n",
    "    # Check if the predicted label contains the word \"healthy\"\n",
    "    is_healthy = \"healthy\" in disease_name.lower()\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"disease\": disease_name,\n",
    "        \"is_healthy\": is_healthy,\n",
    "        \"confidence\": round(confidence * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ecc9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.921373188495636\n",
      "Final Validation Accuracy: 0.9139970541000366\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Training Accuracy:\", history.history[\"accuracy\"][-1])\n",
    "print(\"Final Validation Accuracy:\", history.history[\"val_accuracy\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86cd1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k(img_path, k=3, min_conf=0.05):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    preds = model.predict(img_array)[0]\n",
    "    top_indices = preds.argsort()[-k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        conf = float(preds[idx])\n",
    "        if conf >= min_conf:\n",
    "            results.append({\n",
    "                \"disease\": label_map[idx],\n",
    "                \"confidence\": round(conf * 100, 2)\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d15fa5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_severity_from_heatmap(heatmap_path):\n",
    "    heatmap = cv2.imread(heatmap_path)\n",
    "    heatmap_gray = cv2.cvtColor(heatmap, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold to isolate \"hot\" infected zones\n",
    "    _, binary_map = cv2.threshold(heatmap_gray, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    infected_area = np.sum(binary_map == 255)\n",
    "    total_area = binary_map.shape[0] * binary_map.shape[1]\n",
    "\n",
    "    ratio = infected_area / total_area\n",
    "\n",
    "    if ratio < 0.10:\n",
    "        severity = \"Low\"\n",
    "    elif ratio < 0.40:\n",
    "        severity = \"Medium\"\n",
    "    else:\n",
    "        severity = \"High\"\n",
    "\n",
    "    return {\n",
    "        \"severity\": severity,\n",
    "        \"infected_ratio\": round(ratio * 100, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7409b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remediation_plan(disease, severity):\n",
    "    if disease not in REMEDIATION_DB:\n",
    "        return {\n",
    "            \"message\": \"No remediation data available for this disease.\",\n",
    "            \"escalate\": True\n",
    "        }\n",
    "\n",
    "    data = REMEDIATION_DB[disease]\n",
    "\n",
    "    if severity == \"Low\":\n",
    "        treatment = data[\"organic\"]\n",
    "        urgency = \"Monitor & apply organic remedies.\"\n",
    "    elif severity == \"Medium\":\n",
    "        treatment = data[\"organic\"] + data[\"chemical\"]\n",
    "        urgency = \"Apply combined treatment.\"\n",
    "    else:\n",
    "        treatment = data[\"chemical\"]\n",
    "        urgency = \"Immediate chemical treatment required.\"\n",
    "\n",
    "    escalate = severity == \"High\"\n",
    "\n",
    "    return {\n",
    "        \"urgency\": urgency,\n",
    "        \"treatment_steps\": treatment,\n",
    "        \"prevention_tips\": data[\"prevention\"],\n",
    "        \"safety_warnings\": data[\"safety\"],\n",
    "        \"escalate_to_expert\": escalate\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a73f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_predict_with_heatmap(img_path):\n",
    "    result = predict_disease(img_path)\n",
    "\n",
    "    if result[\"status\"] != \"success\":\n",
    "        return result\n",
    "\n",
    "    heatmap_path = generate_gradcam(model, img_path)\n",
    "    severity_info = estimate_severity_from_heatmap(heatmap_path)\n",
    "\n",
    "    remediation = get_remediation_plan(\n",
    "        result[\"disease\"],\n",
    "        severity_info[\"severity\"]\n",
    "    )\n",
    "\n",
    "    result[\"heatmap\"] = heatmap_path\n",
    "    result[\"severity\"] = severity_info[\"severity\"]\n",
    "    result[\"infected_ratio\"] = severity_info[\"infected_ratio\"]\n",
    "    result[\"remediation\"] = remediation\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4db4e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_result(result):\n",
    "    print(\"\\n========== AI CROP DIAGNOSIS REPORT ==========\\n\")\n",
    "\n",
    "    print(f\"Status           : {result.get('status')}\")\n",
    "    print(f\"Disease          : {result.get('disease')}\")\n",
    "    print(f\"Healthy          : {result.get('is_healthy')}\")\n",
    "    print(f\"Confidence       : {result.get('confidence')} %\")\n",
    "    print(f\"Severity         : {result.get('severity')}\")\n",
    "    print(f\"Infected Ratio   : {result.get('infected_ratio')} %\")\n",
    "\n",
    "    if \"remediation\" in result:\n",
    "        r = result[\"remediation\"]\n",
    "\n",
    "        print(\"\\n--- Remediation Plan ---\")\n",
    "        print(f\"Urgency          : {r.get('urgency')}\")\n",
    "\n",
    "        print(\"\\nTreatment Steps:\")\n",
    "        for i, step in enumerate(r.get(\"treatment_steps\", []), 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "\n",
    "        print(\"\\nPrevention Tips:\")\n",
    "        for tip in r.get(\"prevention_tips\", []):\n",
    "            print(f\"  - {tip}\")\n",
    "\n",
    "        print(\"\\nSafety Warnings:\")\n",
    "        for warn in r.get(\"safety_warnings\", []):\n",
    "            print(f\"  âš  {warn}\")\n",
    "\n",
    "        print(f\"\\nEscalate to Expert : {'Yes' if r.get('escalate_to_expert') else 'No'}\")\n",
    "\n",
    "    print(\"\\n=============================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "--- Final Prediction with Severity ---\n",
      "{'status': 'success', 'disease': 'Potato___Early_blight', 'is_healthy': False, 'confidence': 99.99, 'heatmap': 'heatmap_output.jpg', 'severity': 'Low', 'infected_ratio': np.float64(0.69), 'remediation': {'urgency': 'Monitor & apply organic remedies.', 'treatment_steps': ['Remove infected leaves', 'Spray neem oil (3 ml/L)', 'Improve air circulation'], 'prevention_tips': ['Avoid overhead irrigation', 'Use disease-resistant varieties', 'Crop rotation'], 'safety_warnings': ['Wear gloves & mask', 'Do not spray during noon', 'Keep away from children'], 'escalate_to_expert': False}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    TEST_IMAGE = r'0a8a68ee-f587-4dea-beec-79d02e7d3fa4___RS_Early.B 8461.JPG'\n",
    "\n",
    "    result = full_predict_with_heatmap(TEST_IMAGE)\n",
    "    pretty_print_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496158a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0627a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
